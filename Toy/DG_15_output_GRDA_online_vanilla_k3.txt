class KSubsetDataLoader:
    def __init__(self, dataset, batch_size, k, shuffle=True, drop_last=False):
        self.dataset = dataset
        self.batch_size = batch_size
        self.k = k
        self.shuffle = shuffle
        self.drop_last = drop_last

        self.num_domains = len(dataset.datasets)
        assert self.num_domains % k == 0, "Number of subsets must be divisible by k"
        self.num_groups = self.num_domains // k
        self.n = len(dataset.datasets[0])
        
        # Calculate number of batches per group
        if drop_last:
            self.batches_per_domain = self.n // batch_size
        else:
            self.batches_per_domain = (self.n + batch_size - 1) // batch_size
        
        self.num_batches = self.num_groups * self.batches_per_domain

    def __iter__(self):
        m = len(self.dataset.datasets)
        k = self.k
        n = self.n
        batch_size = self.batch_size

        subset_indices = list(np.arange(m))
        check_dict = {i:0 for i in subset_indices}
        groups = []
        while len(subset_indices):
            selected = random.sample(subset_indices, k)
            groups.append(np.array(selected))
            for elem in selected:
                check_dict[elem] += 1
                if check_dict[elem] == self.batches_per_domain:
                    subset_indices.remove(elem)
        
        all_batches = []
        indices = [np.arange(n) for _ in range(m)]
        if self.shuffle:
            for elem in indices:
                np.random.shuffle(elem)
        for group in groups:
            # Split into batches
            batches = []
            for j in group:
                batch_indices = indices[j][:batch_size]
                indices[j] = indices[j][batch_size:]
                
                batches.append(batch_indices)
            
            # Record group and batch indices
            all_batches.extend([(group, batches)])
        # print(f"before {len(all_batches)}, {all_batches}")
        # print('ziyan', len(all_batches), len(all_batches[0][1]))
        if self.shuffle:
            np.random.shuffle(all_batches)
        # print(f"after {len(all_batches)}, {all_batches}")
        # exit(0)
        for group, batch_indices in all_batches:
            # Collect data for this batch
            batch_data = []
            group_argsorted = np.argsort(group)
            for idx in range(k):
                this_i = group[group_argsorted[idx]]
                this_batch_indices = batch_indices[group_argsorted[idx]]
                sample = [self.dataset.datasets[this_i][i] for i in this_batch_indices]
                batch_data.append(sample)
            yield self.collate_fn(batch_data)

    @staticmethod
    def collate_fn(batch):
        """batch 结构： [ K个[], 每个[] 32个tuple, 每个tuple (x,y,domain)]"""
        # print(batch[0])
        # print(len(batch), len(batch[0]), batch[0][0][0].shape)
        
        # 将原始batch结构转换为三维张量
        data = torch.stack([
            torch.stack([torch.from_numpy(s[0]) for s in sample])
            for sample in batch
        ])  # (batch_size, k, features)
        
        labels = torch.stack([
            torch.LongTensor([s[1] for s in sample])
            for sample in batch
        ])  # (batch_size, k)
        
        domains = torch.stack([
            torch.LongTensor([s[2] for s in sample])
            for sample in batch
        ])  # (batch_size, k)

        # print(data.shape, labels.shape, domains.shape)
        # print(domains)
        # 按子数据集拆分并重组
        k = data.shape[0]
        output = []
        for i in range(k):
            output.append((
                data[i, :, :],    # (batch_size, features)
                labels[i, :],     # (batch_size,)
                domains[i, :]     # (batch_size,)
            ))
        # print(output)
        # print(len(output), len(output[0]), output[0][0].shape, output[0][1].shape, output[0][2].shape)
        return output

    def __len__(self):
        return self.num_batches








    def __optimize_DUZF__(self):
        self.train()

        self.optimizer_UZF.zero_grad()

        gradient_mask = self.domain_sel_broadcast.unsqueeze(-1)

        # - E_q[log q(u|x)]
        # u is multi-dimensional
        # loss_q_u_x = torch.mean((0.5 * flat(self.u_log_var_seq)).sum(1), dim=0)
        loss_q_u_x = torch.mean((0.5 * flat(self.u_log_var_seq * gradient_mask)).sum(1), dim=0)

        # - E_q[log q(z|x,u)]
        # remove all the losses about log var and use 1 as var
        # loss_q_z_x_u = torch.mean((0.5 * flat(self.q_z_log_var_seq)).sum(1),dim=0)
        loss_q_z_x_u = torch.mean((0.5 * flat(self.q_z_log_var_seq * gradient_mask)).sum(1), dim=0)

        # E_q[log p(z|x,u)]
        # first one is for normal
        loss_p_z_x_u = -0.5 * flat(self.p_z_log_var_seq * gradient_mask) - 0.5 * (
            torch.exp(flat(self.q_z_log_var_seq * gradient_mask)) +
            (flat(self.q_z_mu_seq * gradient_mask) - flat(self.p_z_mu_seq * gradient_mask))**2) / flat(
                torch.exp(self.p_z_log_var_seq * gradient_mask))

        loss_p_z_x_u = torch.mean(loss_p_z_x_u.reshape(self.num_domain * self.tmp_batch_size, -1).sum(1), dim=0)

        # print(f"the shape for y_seq:{self.y_seq.shape}, domain mask {self.domain_mask.shape}")
        # print((self.domain_mask == 1).to(self.y_seq.device))
        # print(torch.isin(torch.arange(self.domain_mask.shape[0]).to(self.y_seq.device), self.domain_sel))
        mask_comb = (self.domain_mask == 1).unsqueeze(-1) & (self.domain_sel_broadcast == 1)
        # print(f"{self.y_seq[mask1 & mask2].shape}")
        # exit(0)
        # E_q[log p(y|z)]
        # y_seq_source = self.y_seq[self.domain_mask == 1]
        # f_seq_source = self.f_seq[self.domain_mask == 1]
        y_seq_source = self.y_seq * mask_comb
        f_seq_source = self.f_seq * mask_comb.unsqueeze(-1)

        if y_seq_source.shape[0] == 0:
            loss_p_y_z = torch.tensor([0]).to(y_seq_source.device)
        else:
            loss_p_y_z = -F.nll_loss(
                flat(f_seq_source).squeeze(), flat(y_seq_source))
        
        # E_q[log p(\beta|\alpha)]
        # assuming alpha mean = 0
        # print(self.beta_log_var_seq.shape)  # num_domain, 2
        var_beta_mask = (self.domain_sel_broadcast.sum(dim=1) != 0).unsqueeze(-1)
        # if var_beta_mask.sum() != self.opt.k: 
        #     print(var_beta_mask)
        #     exit(0)
        var_beta = torch.exp(self.beta_log_var_seq * var_beta_mask) 
        # To reproduce the exact result of our experiment, use the following line to replace the loss_beta_alpha:
        loss_beta_alpha = -torch.mean((var_beta**2).sum(dim=1))
        # Actually the previous line is wrong because based on our formula, it should be var_beta, not var_beta**2.
        # However, all our parameter tunning is based on the previous one. Thus, to ensure that you can reproduce our results, please use the previous line.
        # The correct line should be: 
        # loss_beta_alpha = -torch.mean(var_beta.sum(dim=1))
        # Uncomment the previous line to use the correct formula.

        # loss for u and beta
        # log p(u|beta)
        
        beta_t = self.beta_U_seq.unsqueeze(dim=1).expand(
            -1, self.tmp_batch_size, -1) 
        # now beta_t is domain x batch x domain idx dim
        # print(beta_t.shape)  # [15,32,4]
        loss_p_u_beta = ((self.u_seq - beta_t)**2 * gradient_mask).sum(2) 
        loss_p_u_beta = -torch.mean(loss_p_u_beta) 

        # concentrate loss
        loss_u_concentrate = self.contrastive_loss(self.u_con_seq)

        # reconstruction loss (p(x|u))
        loss_p_x_u = ((flat(self.x_seq * gradient_mask) - flat(self.r_x_seq * gradient_mask))**2).sum(1)
        loss_p_x_u = -torch.mean(loss_p_x_u) 

        # gan loss (adversarial loss)
        if self.opt.lambda_gan != 0:
            loss_E_gan = -self.loss_D
        else:
            loss_E_gan = torch.tensor(0,
                                      dtype=torch.double,
                                      device=self.opt.device)
            
        loss_E = loss_E_gan * self.opt.lambda_gan + self.opt.lambda_u_concentrate * loss_u_concentrate - (
            self.opt.lambda_reconstruct * loss_p_x_u + self.opt.lambda_beta *
            loss_p_u_beta + self.opt.lambda_beta_alpha * loss_beta_alpha +
            loss_p_y_z + loss_q_u_x + loss_q_z_x_u + loss_p_z_x_u)

        self.optimizer_D.zero_grad()
        self.loss_D.backward(retain_graph=True)
        self.optimizer_UZF.zero_grad()
        loss_E.backward()

        self.optimizer_D.step()
        self.optimizer_UZF.step()
        flag = False
        # if self.epoch > 79:
        #     print(self.epoch)
        #     if np.isnan(loss_p_z_x_u.item()):
        #         print(flat(self.p_z_log_var_seq).mean())
        #         print(torch.exp(flat(self.q_z_log_var_seq)).mean())  # inf
        #         print(((flat(self.q_z_mu_seq) - flat(self.p_z_mu_seq))**2).mean())
        #         print(flat(torch.exp(self.p_z_log_var_seq)).mean())  # inf

        #     for elem in [self.loss_D.item(), -loss_p_y_z.item(), loss_q_u_x.item(), 
        #                  loss_q_z_x_u.item(), loss_p_z_x_u.item(), loss_u_concentrate.item(), -loss_p_x_u.item(), 
        #                  -loss_p_u_beta.item(), -loss_beta_alpha.item()]:
        #         if np.isnan(elem):
        #             flag=True
        #             print(self.epoch, elem)
        #     if flag:
        #         for elem in [self.loss_D.item(), -loss_p_y_z.item(), loss_q_u_x.item(), 
        #                  loss_q_z_x_u.item(), loss_p_z_x_u.item(), loss_u_concentrate.item(), -loss_p_x_u.item(), 
        #                  -loss_p_u_beta.item(), -loss_beta_alpha.item()]:
        #             print(elem)
        #         exit(0)

        return self.loss_D.item(), -loss_p_y_z.item(), loss_q_u_x.item(
        ), loss_q_z_x_u.item(), loss_p_z_x_u.item(), loss_u_concentrate.item(
        ), -loss_p_x_u.item(), -loss_p_u_beta.item(), -loss_beta_alpha.item()